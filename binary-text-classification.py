# -*- coding: utf-8 -*-
"""proiect-retele.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O6u1s2JaaVOZ5MiFot_v6Zn7F2ujv_5n
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from scipy import sparse
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

# pentru datele de training si validation luam doar coloana a doua
training_data = np.genfromtxt('train_samples.txt',comments='None', dtype='str', delimiter='\t', encoding='utf-8')[:, 1]
training_labels = np.loadtxt('train_labels.txt', dtype='int')[:, 1]

validation_data = np.genfromtxt('validation_samples.txt',comments='None', dtype='str', delimiter='\t', encoding='utf-8')[:, 1]
validation_labels = np.loadtxt('validation_labels.txt', dtype='int')[:, 1]

test_data = np.genfromtxt('test_samples.txt',comments='None', dtype='str', delimiter='\t', encoding='utf-8')

transformer = TfidfVectorizer(ngram_range=(6, 6), analyzer='char_wb', lowercase=False, norm='l2')

# keras accepta doar matrici dense nu sparse
training_features = transformer.fit_transform(training_data)
training_features = sparse.csr_matrix.todense(training_features)

validation_features = transformer.transform(validation_data)
validation_features = sparse.csr_matrix.todense(validation_features)

model = Sequential()

activation = 'sigmoid'

model.add(layers.Dense(units=4, input_dim=(training_features.shape[1])))
model.add(layers.Activation(activation))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(units=8))
model.add(layers.Activation(activation))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(units=4))
model.add(layers.Activation(activation))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(units=1))
model.add(layers.Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(training_features, training_labels,
                    epochs=100, batch_size=100,
                    validation_data=(validation_features, validation_labels))

predictions_validation = model.predict_classes(validation_features)

print(metrics.classification_report(validation_labels, predictions_validation))

print(metrics.confusion_matrix(validation_labels, predictions_validation))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training loss', 'Validation loss'], loc='upper left')
plt.show()

test_features = transformer.transform(test_data[:, 1])
test_features = sparse.csr_matrix.todense(test_features)

predictions = model.predict_classes(test_features)

data = {'id':test_data[:, 0], 'label':predictions.flatten()}

df = pd.DataFrame(data)

print(df)

df.to_csv('predictii.csv', index=False)